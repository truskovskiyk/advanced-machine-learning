Homework part I
Tabular crossentropy method
You may have noticed that the taxi problem quickly converges from -100 to a near-optimal score and then descends back into -50/-100. This is in part because the environment has some innate randomness. Namely, the starting points of passenger/driver change from episode to episode.

Tasks
1.1 (1 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.
1.2 (2 pts) Tune the algorithm to end up with positive average score.
It's okay to modify the existing code.

<Describe what you did here.  Preferably with plot/report to support it.>

Homework part II
Deep crossentropy method
By this moment you should have got enough score on CartPole-v0 to consider it solved (see the link). It's time to upload the result and get to something harder.

if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.
Tasks
2.1 (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2.
For MountainCar, get average reward of at least -150
For LunarLander, get average reward of at least +50
For any environment, upload it to gym and post url in your anytask form.
See the tips section below, it's kinda important. Note: If your agent is below the target score, you'll still get most of the points depending on the result, so don't be afraid to submit it.

2.2 (bonus: 4++ pt) Devise a way to speed up training at least 2x against the default version
Obvious improvement: use joblib
Try re-using samples from 3-5 last iterations when computing threshold and training
Experiment with amount of training iterations and learning rate of the neural network (see params)
Please list what you did in anytask submission form
Tips
Gym page: mountaincar, lunarlander
Sessions for MountainCar may last for 10k+ ticks. Make sure t_max param is at least 10k.
Also it may be a good idea to cut rewards via ">" and not ">=". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold fails cut off bad sessions whule R > threshold works alright.
issue with gym: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified t_max, and if it isn't, try env = gym.make("MountainCar-v0").env or otherwise get rid of TimeLimit wrapper.
If you use old swig lib for LunarLander-v2, you may get an error. See this issue for solution.
If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)
20-neuron network is probably not enough, feel free to experiment.
Please upload the results to openai gym and send links to all submissions in the e-mail
Bonus tasks
2.3 bonus Try to find a network architecture and training params that solve both environments above (Points depend on implementation. If you attempted this task, please mention it in anytask submission.)

2.4 bonus Solve continuous action space task with MLPRegressor or similar.

MountainCarContinuous-v0, LunarLanderContinuous-v2
4 points for solving. Slightly less for getting some results below solution threshold. Note that discrete and continuous environments may have slightly different rules aside from action spaces.
If you're still feeling unchallenged, consider the project (see other notebook in this folder).


TASKS FOR ME

1) CartPole-v0 - load to gym
2) MountainCar-v0 -150 - load to gym
3) LunarLander-v2 +50 - load to gym
4) MountainCarContinuous-v0 - load to gym
5) LunarLanderContinuous-v2 - load to gym
6) speed up

PROJECT
1) Evolution Strategies


